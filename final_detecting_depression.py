# -*- coding: utf-8 -*-
"""Final_Detecting_Depression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MIkf8XcMTbnkKlbZGMboSnXuZdG4jG1h
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Preprocessing

### Importing Libraries
"""

!pip install pyngrok
!pip install flask-ngrok
!pip install flask==0.12.2  # for the app

!pip install chart_studio

#importing all libraries
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.sequence import pad_sequences
import os
import re
import numpy as np
import pandas as pd
import chart_studio.plotly as py
import plotly.graph_objs as go
import itertools
from scipy import stats
from ast import literal_eval
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split

from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Input, Activation, GlobalAveragePooling1D, Flatten, Concatenate, Conv1D, MaxPooling1D,Bidirectional,TimeDistributed,Reshape,Conv2D,MaxPool2D
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import concatenate
from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adam
from tensorflow.keras.preprocessing.text import one_hot, text_to_word_sequence, Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.utils import plot_model

import fnmatch

import warnings

import string
from pathlib import Path
from random import shuffle
from ast import literal_eval

warnings.filterwarnings('ignore')

#!pip install nltk
import nltk

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

from nltk.stem.wordnet import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

import gensim
import json


import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import plot_model
import tensorflow.keras.utils
from tensorflow.keras import utils as np_utils

#Keras Tokenizer just replaces certain punctuation characters and splits on the remaining space character.
#NLTK Tokenizer uses the Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.

WINDOWS_SIZE = 10
labels=['none','mild','moderate','moderately severe', 'severe']
num_classes = len(labels)

"""### Creating a dataframe from the transcript file"""

def transcripts_to_dataframe(directory):
    rows_list = []

    filenames = os.listdir(directory)

    if ".DS_Store" in filenames:
        filenames.remove(".DS_Store")

    for filename in filenames:
        transcript_path = os.path.join(directory, filename)
        transcript = pd.read_csv(transcript_path, sep='\t')
        m = re.search("(\d{3})_TRANSCRIPT.csv", filename)
        if m:
            person_id = m.group(1)
            p = {}
            question = ""
            answer = ""
            lines = len(transcript)
            for i in range(0, lines):
                row = transcript.iloc[i]
                if (row["speaker"] == "Ellie") or (i == lines - 1):
                    p["personId"] = person_id
                    if "(" in str(question):
                        question = question[question.index("(") + 1:question.index(")")]
                    p["question"] = question
                    p["answer"] = answer
                    if question != "":
                        rows_list.append(p)
                    p = {}
                    answer = ""
                    question = row["value"]
                else:
                    answer = str(answer) + " " + str(row["value"])

    all_participants = pd.DataFrame(rows_list, columns=['personId', 'question', 'answer'])
    all_participants.to_csv(directory + 'all.csv', sep=',')
    print("File was created")
    return all_participants

#loading the data
data_path = "/content/drive/My Drive/transcripts/data/"
all_participants = transcripts_to_dataframe(data_path)

all_participants.head(20)

"""### Removing the stopwords and cleaning the data"""

#https://www.kaggle.com/currie32/the-importance-of-cleaning-text  the Importance of cleaning data !

def text_to_wordlist(text, remove_stopwords=True, stem_words=False):
    # Clean the text, with the option to remove stopwords and to stem words.

    # Convert words to lower case and split them
    text = text.lower().split()

    # Optionally, remove stop words
    if remove_stopwords:
        stops = stopwords.words("english")
        text = [wordnet_lemmatizer.lemmatize(w) for w in text if not w in stops ]
        text = [w for w in text if w != "nan" ]
    else:
        text = [wordnet_lemmatizer.lemmatize(w) for w in text]
        text = [w for w in text if w != "nan" ]

    text = " ".join(text)

    # Clean the text
    text = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", text)
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "cannot ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r",", " ", text)
    text = re.sub(r"\.", " ", text)
    text = re.sub(r"!", " ! ", text)
    text = re.sub(r"\/", " ", text)
    text = re.sub(r"\^", " ^ ", text)
    text = re.sub(r"\+", " + ", text)
    text = re.sub(r"\-", " - ", text)
    text = re.sub(r"\=", " = ", text)

    text = re.sub(r"\<", " ", text)
    text = re.sub(r"\>", " ", text)

    text = re.sub(r"'", " ", text)
    text = re.sub(r"(\d+)(k)", r"\g<1>000", text)
    text = re.sub(r":", " : ", text)
    text = re.sub(r" e g ", " eg ", text)
    text = re.sub(r" b g ", " bg ", text)
    text = re.sub(r" u s ", " american ", text)
    text = re.sub(r"\0s", "0", text)
    text = re.sub(r" 9 11 ", "911", text)
    text = re.sub(r"e - mail", "email", text)
    text = re.sub(r"j k", "jk", text)
    text = re.sub(r"\s{2,}", " ", text)

    # Optionally, shorten words to their stems
    if stem_words:
        text = text.split()
        stemmer = SnowballStemmer('english')
        stemmed_words = [stemmer.stem(word) for word in text]
        text = " ".join(stemmed_words)

    # Return a list of words
    return(text)

import nltk
nltk.download('omw-1.4')

#creating a corpus with the words from the answers without stopwords given by the patients
all_participants_mix = all_participants.copy() # However, if you need the original list unchanged when the new list is modified, you can use copy() method. This is called shallow copy.
all_participants_mix['answer'] = all_participants_mix.apply(lambda row: text_to_wordlist(row.answer).split(), axis=1)

##creating a corpus with the words from the answers withstopwords given by the patients
# stopwords are ={what is , have,}
all_participants_mix_stopwords = all_participants.copy()
all_participants_mix_stopwords['answer'] = all_participants_mix_stopwords.apply(lambda row: text_to_wordlist(row.answer, remove_stopwords=False).split(), axis=1)

words = [w for w in all_participants_mix['answer'].tolist()]
words = set(itertools.chain(*words)) #chain('ABC', 'DEF') --> A B C D E F
vocab_size = len(words)

"""### Getting the top common words used by the patients"""

words

words_stop = [w for w in all_participants_mix_stopwords['answer'].tolist()]
words_stop = set(itertools.chain(*words_stop))
vocab_size_stop = len(words_stop)

words_stop

windows_size = WINDOWS_SIZE
tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(all_participants_mix['answer']) # fit_on_texts creates the vocabulary index based on word frequency.
#The cat sat on the mat." It will create a dictionary s.t. word_index["the"] = 1; word_index["cat"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word

tokenizer.fit_on_sequences(all_participants_mix['answer']) #texts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary.

all_participants_mix['t_answer'] = tokenizer.texts_to_sequences(all_participants_mix['answer'])
all_participants_mix.head(15)

#   why are the output as numbers when text_to_sequences is called?
# the Tokenizer stores everything in the word_index during fit_on_texts. Then, when calling the texts_to_sequences method, only the top num_words are considered.

import pickle
with open('./tokenizer.pickle', 'wb') as f:
    pickle.dump(tokenizer, f)

windows_size = WINDOWS_SIZE
tokenizer = Tokenizer(num_words=vocab_size_stop)
tokenizer.fit_on_texts(all_participants_mix_stopwords['answer'])
tokenizer.fit_on_sequences(all_participants_mix_stopwords['answer'])

all_participants_mix_stopwords['t_answer'] = tokenizer.texts_to_sequences(all_participants_mix_stopwords['answer'])
all_participants_mix_stopwords.head(15)

word_index = tokenizer.word_index
word_size = len(word_index)
print(word_index["sad"])

"""# Data Augmentation of the tokenized words to improve model performance"""

windows_size = WINDOWS_SIZE
cont = 0
word_index = tokenizer
phrases_lp = pd.DataFrame(columns=['personId','answer','t_answer'])
answers = all_participants_mix.groupby('personId').agg('sum')

for p in answers.iterrows():
    words = p[1]["answer"]
    size = len(words)
    word_tokens = p[1]["t_answer"]

    for i in range(size):
        sentence = words[i:min(i+windows_size,size)]
        tokens = word_tokens[i:min(i+windows_size,size)]
        phrases_lp.loc[cont] = [p[0], sentence, tokens]
        cont = cont + 1

phrases_lp.head()
phrases_lp.to_csv('/content/drive/My Drive/transcripts/phrases_lp.csv', sep='\t')
print("File was created")

phrases_lp["t_answer"] = pad_sequences(phrases_lp["t_answer"], value=0, padding="post", maxlen=windows_size).tolist()
phrases_lp.drop(phrases_lp[phrases_lp["t_answer"].map(len) > 10].index, inplace = True)

#phrases_lp_stop.head()
#phrases_lp_stop.to_csv('/content/drive/My Drive/transcripts/phrases_lp_stop.csv', sep='\t')
#print("File was created")

"""### Loading the data augmented dataset"""

phrases_lp = pd.read_csv('/content/drive/My Drive/transcripts/phrases_lp.csv', sep='\t', converters={"t_answer": literal_eval})
phrases_lp.head(25)

"""### Loading the train, validation and test data containing the PHQ Scores"""

from tensorflow.keras.utils import to_categorical
def load_avec_dataset_file(path,score_column):
    ds = pd.read_csv(path, sep=',')
    ds['level'] = pd.cut(ds[score_column], bins=[-1,0,5,10,15,25], labels=[0,1,2,3,4])  #cut function used to segregate array into bins 5 levels - 'none','mild','moderate','moderately severe', 'severe'
    ds['PHQ8_Score'] = ds[score_column]
    ds['cat_level'] = to_categorical(ds['level'], num_classes).tolist() #categorical levels
    ds = ds[['Participant_ID', 'level', 'cat_level', 'PHQ8_Score','Gender']]
    ds = ds.astype({"Participant_ID": float, "level": int, 'PHQ8_Score': int})
    return ds

train = load_avec_dataset_file('/content/drive/My Drive/transcripts/train_split_Depression_AVEC2017.csv','PHQ8_Score')
dev = load_avec_dataset_file('/content/drive/My Drive/transcripts/dev_split_Depression_AVEC2017.csv','PHQ8_Score')
test = load_avec_dataset_file('/content/drive/My Drive/transcripts/full_test_split.csv','PHQ8_Score')
print("Size: train= {}, dev= {}, test={}".format(len(train), len(dev), len(test)))
train.head()

ds_total = pd.concat([dev,train,test])
total_phq8 = len(ds_total)
print("Total size = {}".format(total_phq8))

ds_total

ds_total.to_csv('/content/drive/My Drive/transcripts/ds_total.csv', sep='\t')
print("File was created")

bins=[-1,0,5,10,15,25]
plt.figure()
plt.hist(ds_total["PHQ8_Score"], rwidth=0.6, bins=5)
plt.xlabel('PHQ8 score')
plt.ylabel('Number of participants')
plt.show()
plt.savefig('/content/drive/My Drive/transcripts/bins.png')

"""### Splitting the dataset and grouping them based on the 5 different level"""

def split_by_phq_level(ds):
    none_ds = ds[ds['level']==0]
    mild_ds = ds[ds['level']==1]
    moderate_ds = ds[ds['level']==2]
    moderate_severe_ds = ds[ds['level']==3]
    severe_ds = ds[ds['level']==4]
    return (none_ds, mild_ds, moderate_ds, moderate_severe_ds, severe_ds)

none_ds, mild_ds, moderate_ds, moderate_severe_ds, severe_ds = split_by_phq_level(ds_total)
print("Quantity per none_ds: {}, mild_ds: {}, moderate_ds {}, moderate_severe_ds: {}, severe_ds {}".format(len(none_ds), len(mild_ds), len(moderate_ds), len(moderate_severe_ds), len(severe_ds)))

b_none_ds = ds_total[ds_total['level']==0]
b_mild_ds = ds_total[ds_total['level']==1].sample(26)
b_moderate_ds = ds_total[ds_total['level']==2].sample(26)
b_moderate_severe_ds = ds_total[ds_total['level']==3]
b_severe_ds = ds_total[ds_total['level']==4]

ds_total_b = pd.concat([b_none_ds, b_mild_ds, b_moderate_ds, b_moderate_severe_ds, b_severe_ds])

ds_= ds_total_b.to_csv('ds_total_b.csv')

"""### Merging the phrases_lp (consisting of the transcripts) and ds_total consisting of PHQ Score"""

ds_lp = pd.merge(ds_total, phrases_lp,left_on='Participant_ID', right_on='personId')
ds_lp.drop(ds_lp[ds_lp["t_answer"].map(len) < 10].index, inplace = True)
ds_lp_b = pd.merge(ds_total_b, phrases_lp,left_on='Participant_ID', right_on='personId')
ds_lp_b.drop(ds_lp_b[ds_lp_b["t_answer"].map(len) < 10].index, inplace = True)

(len(ds_lp))

"""### Splitting ds_total into train, dev and test in the ratio of 70:14:16"""

def distribute_instances(ds, split_in = [70,14,16]):
    ds_shuffled = ds.sample(frac=1)
    none_ds, mild_ds, moderate_ds, moderate_severe_ds, severe_ds = split_by_phq_level(ds_shuffled)
    eq_ds = dict()
    prev_none = prev_mild = prev_moderate = prev_moderate_severe = prev_severe = 0
    split = split_in
    for p in split:
        last_none = min(len(none_ds), prev_none + round(len(none_ds) * p/100))
        last_mild = min(len(mild_ds), prev_mild + round(len(mild_ds) * p/100))
        last_moderate = min(len(moderate_ds), prev_moderate + round(len(moderate_ds) * p/100))
        last_moderate_severe = min(len(moderate_severe_ds), prev_moderate_severe + round(len(moderate_severe_ds) * p/100))
        last_severe = min(len(severe_ds), prev_severe + round(len(severe_ds) * p/100))
        eq_ds['d'+str(p)] = pd.concat([none_ds[prev_none: last_none], mild_ds[prev_mild: last_mild], moderate_ds[prev_moderate: last_moderate], moderate_severe_ds[prev_moderate_severe: last_moderate_severe], severe_ds[prev_severe: last_severe]])
        prev_none = last_none
        prev_mild = last_mild
        prev_moderate = last_moderate
        prev_moderate_severe = last_moderate_severe
        prev_severe = last_severe
    return (eq_ds['d70'], eq_ds['d14'], eq_ds['d16'])

train_lp, dev_lp, test_lp = distribute_instances(ds_lp)
train_lp_b, dev_lp_b, test_lp_b = distribute_instances(ds_lp_b)

"""### Creating a confusion matrix which consists of the TPs, TNs, FPs and FNs of the predicted labels for the transcripts"""

def confusion_matrix(model, x, y):
    prediction = model.predict(x, batch_size=None, verbose=0, steps=None)
    labels=['none','mild','moderate','moderately severe', 'severe']

    max_prediction = np.argmax(prediction, axis=1)

    max_actual = np.argmax(y, axis=1)

    y_pred = pd.Categorical.from_codes(max_prediction, labels)
    y_actu = pd.Categorical.from_codes(max_actual, labels)

    return pd.crosstab(y_actu, y_pred)

"""### Loading the Glove Embeddings into a file and putting each vector into an np array"""

#embeddings_index = dict()
#f = open('/content/drive/My Drive/glove.6B.100d.txt', encoding="utf8")
#for line in f:
#    values = line.split()
#    word = values[0]
#   coefs = np.asarray(values[1:], dtype='float32')
#    embeddings_index[word] = coefs
#f.close()
!wget http://nlp.stanford.edu/data/glove.6B.zip

!unzip glove*.zip

print('Indexing word vectors.')

embeddings_index = {}
f = open('glove.6B.100d.txt', encoding='utf-8')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s word vectors.' % len(embeddings_index))

"""### Creating a embedding matrix"""

def fill_embedding_matrix(tokenizer):
    vocab_size = len(tokenizer.word_index) # tokenizer.word_index is the list that consist of all the unique words
    embedding_matrix = np.zeros((vocab_size+1, 100)) # creating an embedding matrix
    for word, i in tokenizer.word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
    return embedding_matrix

"""### calling the embedding matrix function for creating vectors for each unique word"""

embedding_matrix_lp = fill_embedding_matrix(tokenizer)
embedding_matrix_lp.shape

train_lp['t_answer']
train_lp.head()

train_a =np.stack(train_lp['t_answer'])
dev_a = np.stack(dev_lp['t_answer'])
train_y = np.stack(train_lp['cat_level'], axis=0)
dev_y = np.stack(dev_lp['cat_level'], axis=0)

dev_a.shape

"""### stacking all the arrays together to form a single array"""

train_a_b = np.stack(train_lp_b['t_answer'], axis=0)
dev_a_b = np.stack(dev_lp_b['t_answer'], axis=0)
train_y_b = np.stack(train_lp_b['cat_level'], axis=0)
dev_y_b = np.stack(dev_lp_b['cat_level'], axis=0)

early_stopping = EarlyStopping(monitor='val_loss', patience=3)

"""### creating function for plotting the loss and accuracies of the models"""

def plot_acc(history, title="Model Accuracy"):
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title(title)
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'], loc='upper left')
    plt.show()

def plot_loss(history, title="Model Loss"):
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title(title)
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'], loc='upper right')
    plt.show()

def plot_compare_losses(history1, history2, name1="Red 1", name2="Red 2", title="Graph title"):
    plt.plot(history1.history['loss'], color="green")
    plt.plot(history1.history['val_loss'], 'r--', color="green")
    plt.plot(history2.history['loss'], color="blue")
    plt.plot(history2.history['val_loss'], 'r--', color="blue")
    plt.title(title)
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train ' + name1, 'Val ' + name1,
                'Train ' + name2, 'Val ' + name2],
               loc='upper right')
    plt.show()

def plot_compare_accs(history1, history2, name1="Red 1",
                      name2="Red 2", title="Graph title"):
    """Compara accuracies de dos entrenamientos con nombres name1 y name2"""
    plt.plot(history1.history['acc'], color="green")
    plt.plot(history1.history['val_acc'], 'r--', color="green")
    plt.plot(history2.history['acc'], color="blue")
    plt.plot(history2.history['val_acc'], 'r--', color="blue")
    plt.title(title)
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train ' + name1, 'Val ' + name1,
                'Train ' + name2, 'Val ' + name2],
               loc='lower right')
    plt.show()

def plot_compare_multiple_metrics(history_array, names, colors, title="Graph title", metric='acc'):
    legend = []
    for i in range(0, len(history_array)):
        plt.plot(history_array[i].history[metric], color=colors[i])
        plt.plot(history_array[i].history['val_' + metric], 'r--', color=colors[i])
        legend.append('Train ' + names[i])
        legend.append('Val ' + names[i])

    plt.title(title)
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.axis
    plt.legend(legend,
               loc='lower right')
    plt.show()

early_stopping = EarlyStopping(monitor='val_loss', patience=3)

"""### answer_emb1 is the input consisting of the vectors having a window size of 10"""

answer_inp = Input(shape=(windows_size, ))
embedding_size_glove = 100
answer_emb1 = Embedding(vocab_size_stop+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)

"""### GLoVE + LSTM MODEL"""

bt = BatchNormalization()(answer_emb1)
lstm = LSTM(embedding_size_glove, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(bt)

dense1 = Dense(units=256, activation="relu")(lstm)
dense2 = Dense(units=256, activation="relu")(dense1)

flatten = Flatten()(dense2)

out = Dense(5,  activation='softmax')(flatten)

model = Model(inputs=[answer_inp], outputs=[out])
model.compile(optimizer="adam", loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

model_glove_lstm_hist = model.fit(train_a, train_y, validation_data=(dev_a, dev_y), epochs=30, batch_size=64, shuffle=True, callbacks=[early_stopping])

#model.save('./model2',save_format='tf')
model.save('model_glove_lstm.h5')

test_a = np.stack(test_lp['t_answer'], axis=0)
test_y = np.stack(test_lp['cat_level'], axis=0)
test_a_b = np.stack(test_lp_b['t_answer'], axis=0)
test_y_b = np.stack(test_lp_b['cat_level'], axis=0)

test_a = np.stack(test_lp['t_answer'], axis=0)
test_y = np.stack(test_lp['cat_level'], axis=0)
test_a_b = np.stack(test_lp_b['t_answer'], axis=0)
test_y_b = np.stack(test_lp_b['cat_level'], axis=0)
df_confusion1 = confusion_matrix(model, test_a, test_y)

df_confusion1

score = model.evaluate(test_a, test_y, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

"""
from sklearn.externals import joblib
joblib.dump(model,"model.pkl"),
"""



"""## **LSTM Model for the balanced dataset**"""

model_glove_lstm_hist_b = model.fit(train_a_b, train_y_b, validation_data=(dev_a_b, dev_y_b), epochs=30, batch_size=64, shuffle=True, callbacks=[early_stopping])

model.save('model_glove_lstm_b.h5')

plot_loss(model_glove_lstm_hist)
plot_acc(model_glove_lstm_hist)
plot_loss(model_glove_lstm_hist_b)
plot_acc(model_glove_lstm_hist_b)

test_a = np.stack(test_lp['t_answer'], axis=0)
test_y = np.stack(test_lp['cat_level'], axis=0)
test_a_b = np.stack(test_lp_b['t_answer'], axis=0)
test_y_b = np.stack(test_lp_b['cat_level'], axis=0)
df_confusion = confusion_matrix(model, test_a_b, test_y_b)

df_confusion

score = model.evaluate(test_a_b, test_y_b, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

#model.save('/content/drive/My Drive/transcripts/model_glove_lstm_b.h5')
#model.save('model_glove_lstm_b.h5')
with open('/content/drive/My Drive/transcripts/model_glove_lstm_b_hist.json', 'w') as f:
    json.dump(str(model_glove_lstm_b_hist.history), f)

def test_model(text,model):
    print(text)
    word_list = text_to_wordlist(text)
    sequences = tokenizer.texts_to_sequences([word_list])
    sequences_input = list(itertools.chain(*sequences))
    sequences_input =  pad_sequences([sequences_input], value=0, padding="post", maxlen=windows_size).tolist()
    print(sequences_input)
    input_a = np.asarray(sequences_input)
    pred = model.predict(input_a, batch_size=None, verbose=0, steps=None)
    predicted_class = np.argmax(pred)
    print(labels[predicted_class])

text= "I want an ice cream and have some fries for lunch"
test_model(text, model)
sen = "I'm afraid of losing my work, I don't have any money"
test_model(sen, model)
sen = "I'm worried about my future, I'm afraid of it"
test_model(sen, model)
sen = "I am a graduate student"
test_model(sen, model)
sen = "I am getting married"
test_model(sen, model)
sen = "This party is great, I know lots of people"
test_model(sen, model)
sen = "I miss my parents, brothers and sisters"
test_model(sen, model)
sen = "I detest my horrible job"
test_model(sen, model)
sen = "I cannot handle this anymore"
test_model(sen, model)

sample=input("Enter your text  \n")
test_model(sample, model)



"""## MODEL 2  glove + 2LSTMs"""

answer_inp = Input(shape=(windows_size, ))
embedding_size_glove = 100
answer_emb1 = Embedding(vocab_size_stop+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)


lstm1 = LSTM(embedding_size_glove, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(answer_emb1)
lstm2 = LSTM(embedding_size_glove, dropout=0.2, recurrent_dropout=0.2)(lstm1)

X = Dropout(0.2)(lstm2)
bt = BatchNormalization()(X)
dense1 = Dense(units=256, activation="relu")(bt)

out = Dense(5,  activation='softmax')(dense1)

model_2lstm = Model(inputs=[answer_inp], outputs=[out])
model_2lstm.compile(optimizer="adam", loss='categorical_crossentropy', metrics=['accuracy'])
model_2lstm.summary()

model_glove_2lstm_b_hist = model_2lstm.fit(train_a_b, train_y_b, validation_data=(dev_a_b, dev_y_b), epochs=30, batch_size=64, shuffle=True, callbacks=[early_stopping])

model_2lstm.save('model_2lstm_b.h5')

plot_loss(model_glove_2lstm_b_hist)
plot_acc(model_glove_2lstm_b_hist)

score = model_2lstm.evaluate(test_a_b, test_y_b, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

df_confusion = confusion_matrix(model_2lstm, test_a_b, test_y_b)

df_confusion

model_2lstm.save('/content/drive/My Drive/transcripts/model_glove_2lstm_b.h5')
model_2lstm.save('model_glove_2lstm_b.h5')
#with open('/content/drive/My Drive/transcripts/model_glove_2lstm_b_hist.json', 'w') as f:
 #   json.dump(model_glove_2lstm_b_hist.history, f)

sen = "All is going right with the party, I'm happy to know new people"
test_model(sen, model_2lstm)
sen = "I want an ice cream and have some fries for lunch"
test_model(sen, model_2lstm)
sen = "I'm afraid of losing my work, I don't have any money"
test_model(sen, model_2lstm)
sen = "I'm worried about my future, I'm afraid of it"
test_model(sen, model_2lstm)
sen = "My father does not love me"
test_model(sen, model_2lstm)

Sample1=input('Enter your message : \n')
test_model(Sample1, model_2lstm)

"""## MODEL 3 Bidirectional LSTM"""

# main model
answer_inp = Input(shape=(windows_size, ))
embedding_size_glove = 100
answer_emb1 = Embedding(vocab_size_stop+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)


bi_lstm =  Bidirectional (LSTM (embedding_size_glove,return_sequences=True,dropout=0.50),merge_mode='concat')(answer_emb1)
model_bi1 = TimeDistributed(Dense(embedding_size_glove,activation='relu'))(bi_lstm) #TimeDistributed method is used to apply a Dense layer to each of the time-steps independently. We used Dropout and l2_reg regularizers to reduce overfitting.
model_bi2 = Flatten()(model_bi1)
model_bi3 = Dense(256,activation='relu')(model_bi2)
output = Dense(5,activation='softmax')(model_bi3)
model_bi = Model(answer_inp,output)
model_bi.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])
model_bi.summary()

model_glove_bilstm = model_bi.fit(train_a_b, train_y_b, validation_data=(dev_a_b, dev_y_b), epochs=30, batch_size=64, shuffle=True, callbacks=[early_stopping])

# evaluate the model
score = model_bi.evaluate(test_a_b, test_y_b, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

model_bi.save('model_bilstm_a_b.h5')

model_bi.save('/content/drive/My Drive/transcripts/model_bilstm_a_b.h5')
model_bi.save('model_bilstm_a_b.h5')
#with open('/content/drive/My Drive/transcripts/model_glove_bilstm.json', 'w') as f:
 #   json.dump(model_glove_bilstm.history, f)

#sample3=input("Enter your text  \n")
#test_model(sample3, model_bi)

"""**Model 4 Using BiGRU**"""

from sklearn.metrics import roc_auc_score
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate
from tensorflow.keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D
from tensorflow.keras.preprocessing import text, sequence
from tensorflow.keras.callbacks import Callback







"""###### AUC - ROC curve is a performance measurement for classification problem at various thresholds settings."""

class RocAucEvaluation(Callback):
    def __init__(self, validation_data=(), interval=1):
        super(Callback, self).__init__()

        self.interval = interval
        self.X_val, self.y_val = validation_data

    def on_epoch_end(self, epoch, logs={}):
        if epoch % self.interval == 0:
            y_pred = self.model.predict(self.X_val, verbose=0)
            score = roc_auc_score(self.y_val, y_pred)
            print("\n ROC-AUC - epoch: %d - score: %.6f \n" % (epoch+1, score))

# main model
answer_inp = Input(shape=(windows_size, ))
embedding_size_glove = 100
answer_emb1 = Embedding(vocab_size_stop+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)

x = SpatialDropout1D(0.2)(answer_emb1)
x = Bidirectional(GRU(embedding_size_glove, return_sequences=True))(x)
avg_pool = GlobalAveragePooling1D()(x)
max_pool = GlobalMaxPooling1D()(x)
conc = concatenate([avg_pool, max_pool])
outp = Dense(5, activation="softmax")(conc)
model_gru = Model(inputs=answer_inp, outputs=outp)
model_gru.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model_gru.summary()

batch_size = 64
epochs = 30
RocAuc = RocAucEvaluation(validation_data=(dev_a,dev_y), interval=1)

hist = model_gru.fit(train_a, train_y, batch_size=batch_size, epochs=epochs, validation_data=(dev_a, dev_y),
                 callbacks=[RocAuc], verbose=2)

model_gru.save('model_gru.h5')

# evaluate the model
score = model_gru.evaluate(test_a_b, test_y_b, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

"""##CNN"""

filter_sizes = [1,2,3,5]
num_filters = 36
answer_inp = Input(shape=(windows_size, ))
embedding_size_glove = 100
answer_emb1 = Embedding(vocab_size_stop+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)
x = Reshape((windows_size, embedding_size_glove, 1))(answer_emb1)
maxpool_pool = []
for i in range(len(filter_sizes)):
  conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embedding_size_glove),kernel_initializer='he_normal', activation='elu')(x)
  maxpool_pool.append(MaxPool2D(pool_size=(windows_size - filter_sizes[i] + 1, 1))(conv))

z = Concatenate(axis=1)(maxpool_pool)
z = Flatten()(z)
z = Dropout(0.1)(z)

outp = Dense(5, activation="softmax")(z)

model = Model(inputs=answer_inp, outputs=outp)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

model_cnn =  model.fit(train_a, train_y, batch_size=64, epochs=30, validation_data=(dev_a, dev_y), verbose=2,callbacks=[early_stopping])

model.save('model_cnn.h5')

# evaluate the model
score = model.evaluate(test_a, test_y, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

"""##CNN + LSTM (Hybrid Model)"""

lstm_cnn = Sequential()
lstm_cnn.add(Embedding(vocab_size_stop+1, 100,weights=[embedding_matrix_lp],input_length=windows_size, trainable=False))
lstm_cnn.add(Dropout(0.2))
lstm_cnn.add(Conv1D(64, 5, activation='relu'))
lstm_cnn.add(MaxPooling1D(pool_size=4))
lstm_cnn.add(LSTM(100))
lstm_cnn.add(Dense(5, activation='softmax'))
lstm_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
## Fit train data
lstm_cnn.summary()



hist = lstm_cnn.fit(train_a_b, train_y_b, validation_data=(dev_a_b, dev_y_b), epochs=30, batch_size=64, shuffle=True, callbacks=[early_stopping])

lstm_cnn.save('model_lstm_cnn.h5')

# evaluate the model
score = lstm_cnn.evaluate(test_a_b, test_y_b, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

"""## BiLSTM + Attention"""

import tensorflow as tf
from keras import backend as K


lstm_units = 128
from tensorflow.keras.layers import RepeatVector,Permute,Multiply
in_text = Input(shape=(windows_size, ))
embedding_layer = Embedding(vocab_size_stop+1,
                        100,
                        weights=[embedding_matrix_lp],
                        input_length=windows_size,
                        trainable=False)
x = embedded_sequences = embedding_layer(in_text)
bilstm = Bidirectional(LSTM(lstm_units,return_sequences=True, dropout=0.3, recurrent_dropout=0.25))(x)
attention = TimeDistributed(Dense(1,activation='tanh'))(bilstm)
attention = Flatten()(attention)
attention = Activation('softmax')(attention)
attention = RepeatVector(2*lstm_units)(attention)
attention = Permute([2, 1])(attention)
sent_representation = Multiply()([bilstm,attention])
out_text = tf.keras.layers.Lambda(lambda xin: K.sum(xin, axis= 1), output_shape=(2*lstm_units,), name='sent')(sent_representation)

output_ = Dense(5, activation='softmax')(out_text)

bilstm_attn = Model(inputs=in_text, outputs= output_)
bilstm_attn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
bilstm_attn.summary()

history = bilstm_attn.fit(train_a_b, train_y_b, validation_data=(dev_a_b, dev_y_b), epochs=30, batch_size=64, shuffle=True)

bilstm_attn.save('model_bilstm_attn.h5')

# evaluate the model
score = bilstm_attn.evaluate(test_a, test_y, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])







#app.py is backend while home.html,predict.html are frontend

from flask import Flask,render_template,request

import itertools
import pickle
#from sklearn.metrics import roc_auc_score
from tensorflow.keras.models import Model,load_model
#from tensorflow.keras.preprocessing.sequence import pad_sequences
#from tensorflow.keras.layers import Input,LSTM,BatchNormalization,Flatten,Bidirectional,SpatialDropout1D,Dense
#from tensorflow.keras.layers import Embedding,GRU,GlobalAveragePooling1D,concatenate,GlobalMaxPooling1D
#from tensorflow.keras.callbacks import Callback,ModelCheckpoint,EarlyStopping
from tensorflow.keras.preprocessing.text import Tokenizer
import numpy as np
#import argparse
import regex as re
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
wordnet_lemmatizer = WordNetLemmatizer()
# we can call a web page using render template
#to initiaize flask class
import pandas as pd
app=Flask(__name__)

windows_size= 10
vocab_size_stop= 1579
embedding_size_glove=100
vocab_size= 1484

#embedding_matrix_lp=np.load('embedding_matr.npy')
def text_to_wordlist(text, remove_stopwords=True, stem_words=False):
    # Clean the text, with the option to remove stopwords and to stem words.

    # Convert words to lower case and split them
    text = text.lower().split()

    # Optionally, remove stop words
    if remove_stopwords:
        stops = stopwords.words("english")
        text = [wordnet_lemmatizer.lemmatize(w) for w in text if not w in stops ]
        text = [w for w in text if w != "nan" ]
    else:
        text = [wordnet_lemmatizer.lemmatize(w) for w in text]
        text = [w for w in text if w != "nan" ]

    text = " ".join(text)

    # Clean the text
    text = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", text)
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "cannot ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r",", " ", text)
    text = re.sub(r"\.", " ", text)
    text = re.sub(r"!", " ! ", text)
    text = re.sub(r"\/", " ", text)
    text = re.sub(r"\^", " ^ ", text)
    text = re.sub(r"\+", " + ", text)
    text = re.sub(r"\-", " - ", text)
    text = re.sub(r"\=", " = ", text)

    text = re.sub(r"\<", " ", text)
    text = re.sub(r"\>", " ", text)

    text = re.sub(r"'", " ", text)
    text = re.sub(r"(\d+)(k)", r"\g<1>000", text)
    text = re.sub(r":", " : ", text)
    text = re.sub(r" e g ", " eg ", text)
    text = re.sub(r" b g ", " bg ", text)
    text = re.sub(r" u s ", " american ", text)
    text = re.sub(r"\0s", "0", text)
    text = re.sub(r" 9 11 ", "911", text)
    text = re.sub(r"e - mail", "email", text)
    text = re.sub(r"j k", "jk", text)
    text = re.sub(r"\s{2,}", " ", text)

    # Optionally, shorten words to their stems
    if stem_words:
        text = text.split()
        stemmer = SnowballStemmer('english')
        stemmed_words = [stemmer.stem(word) for word in text]
        text = " ".join(stemmed_words)

    # Return a list of words
    return(text)

with open('tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)

def app_test(sentence,h5file_paths):
    sentence_stop_removed= text_to_wordlist(sentence)
    answer= tokenizer.texts_to_sequences(sentence_stop_removed)
    answer = list(itertools.chain(*answer))
    answer =  pad_sequences([answer], value=0, padding="post", maxlen=windows_size).tolist()
    answer = np.asarray(answer)

    model1= load_model(h5file_paths[0])
    model2= load_model(h5file_paths[1])
    model3= load_model(h5file_paths[2])
    model4= load_model(h5file_paths[3])
    model5= load_model(h5file_paths[4])
    model6= load_model(h5file_paths[5])
    model7= load_model(h5file_paths[6])
    model8= load_model(h5file_paths[7])

    model_pred1= np.argmax(model1.predict(answer))
    model_pred2= np.argmax(model2.predict(answer))
    model_pred3= np.argmax(model3.predict(answer))
    model_pred4= np.argmax(model4.predict(answer))
    model_pred5= np.argmax(model5.predict(answer))
    model_pred6= np.argmax(model6.predict(answer))
    model_pred7= np.argmax(model7.predict(answer))
    model_pred8= np.argmax(model8.predict(answer))

    model_prediction= np.round_( (model_pred1+model_pred2+model_pred3+model_pred4+model_pred5+model_pred6+model_pred7+model_pred8)/(len(h5file_paths)))
    return model_prediction

!pip install flask-ngrok

!pip install pyngrok
!pip install flask-ngrok
!pip install flask==0.12.2

!ngrok authtoken 2KJXKTal6KOMC6UTNoe8N3IovQ8_7o8YK4pNDaZsox3FzDkde

!pip install pyngrok==4.1.1
!ngrok authtoken '2KJXKTal6KOMC6UTNoe8N3IovQ8_7o8YK4pNDaZsox3FzDkde'

from flask_ngrok import run_with_ngrok
from flask import *
!ngrok authtoken 2KJXKTal6KOMC6UTNoe8N3IovQ8_7o8YK4pNDaZsox3FzDkde

app = Flask(__name__)
run_with_ngrok(app)   #starts ngrok when the app is run
@app.route("/")
def home():
    return render_template("home.html")
    #return "<h1>Running Flask on Google Colab!</h1>"
@app.route("/predict", methods=['GET','POST'])
def predict():
    print("i was here 1")
    model_prediction= 2

    #model_prediction= random.choice([1,2,4])
    if request.method == "POST":

        #print(gru_load_model.input,gru_load_model.output)
        sentence = request.form.get("answer123")

        #file_list=['model_2lstm_b.h5','model_bilstm_a_b.h5','model_cnn.h5','model_glove_2lstm_b.h5','model_glove_lstm.h5','model_glove_lstm_b.h5','model_gru.h5','model_lstm_cnn.h5']
        file_list=['model_2lstm_b.h5','model_bilstm_a_b.h5','model_cnn.h5','model_glove_2lstm_b.h5','model_glove_lstm.h5','model_glove_lstm_b.h5','model_gru.h5','model_lstm_cnn.h5']
        model_prediction= app_test(sentence,file_list)


    return render_template('predict.html',prediction =model_prediction)

!pip uninstall pyngrok==4.1.1

run_with_ngrok(app)
app.run()



